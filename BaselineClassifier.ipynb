{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da46a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] Epoch 01 | train 4.009/0.022 | val 4.087/0.021\n",
      "[Baseline] Epoch 05 | train 3.609/0.046 | val 4.275/0.030\n",
      "[Baseline] Epoch 10 | train 3.441/0.052 | val 4.447/0.030\n",
      "[Baseline] Epoch 15 | train 3.340/0.060 | val 4.551/0.024\n",
      "\n",
      "=== BASELINE TEST RESULTS ===\n",
      "Baseline test loss: 4.1939\n",
      "Baseline test acc : 0.0382\n",
      "(55, 55)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =====================================================\n",
    "# LOAD DATA\n",
    "# =====================================================\n",
    "train_df = pd.read_csv(\"train2.csv\")       # contains features incl. returns\n",
    "labels_df = pd.read_csv(\"labels.csv\")     # contains Backward_Bin, Forward_Bin\n",
    "\n",
    "# Features (remove non-numeric or irrelevant columns as in your main notebook)\n",
    "X = train_df.select_dtypes(include=[np.number]).copy()\n",
    "X = X.drop(columns=[\"Percent_change_forward\", \"Percent_change_backward\"], errors=\"ignore\")\n",
    "\n",
    "# Tomorrow state (0-based)\n",
    "y_all = labels_df[\"Forward_Bin\"].values - 1\n",
    "n_states = int(y_all.max()) + 1\n",
    "\n",
    "# =====================================================\n",
    "# TRAIN/VAL/TEST SPLIT (time-based or random â€” here random for baseline)\n",
    "# =====================================================\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y_all, test_size=0.30, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, shuffle=False)\n",
    "\n",
    "# =====================================================\n",
    "# STANDARDIZE FEATURES\n",
    "# =====================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_val_std   = scaler.transform(X_val)\n",
    "X_test_std  = scaler.transform(X_test)\n",
    "\n",
    "n_features = X_train_std.shape[1]\n",
    "\n",
    "# =====================================================\n",
    "# DATASET & LOADERS\n",
    "# =====================================================\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(SimpleDataset(X_train_std, y_train), batch_size=256, shuffle=True)\n",
    "val_loader   = DataLoader(SimpleDataset(X_val_std,   y_val),   batch_size=512, shuffle=False)\n",
    "test_loader  = DataLoader(SimpleDataset(X_test_std,  y_test),  batch_size=512, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# =====================================================\n",
    "# CLASS WEIGHTS\n",
    "# =====================================================\n",
    "class_counts = np.bincount(y_train, minlength=n_states)\n",
    "class_weights = class_counts.sum() / np.maximum(class_counts, 1)\n",
    "class_weights = class_weights / class_weights.mean()\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# =====================================================\n",
    "# BASELINE MODEL\n",
    "# =====================================================\n",
    "class BaselineNet(nn.Module):\n",
    "    def __init__(self, n_features, n_states):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_states)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "baseline = BaselineNet(n_features, n_states).to(device)\n",
    "optimizer_b = torch.optim.Adam(baseline.parameters(), lr=1e-3)\n",
    "criterion_b = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# =====================================================\n",
    "# TRAINING LOOP\n",
    "# =====================================================\n",
    "def run_epoch(model, loader, train=False):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss, total_correct, total_n = 0, 0, 0\n",
    "\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for Xb, yb in loader:\n",
    "            Xb = Xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            logits = model(Xb)\n",
    "            loss = criterion_b(logits, yb)\n",
    "\n",
    "            if train:\n",
    "                optimizer_b.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_b.step()\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == yb).sum().item()\n",
    "            total_loss += loss.item() * len(Xb)\n",
    "            total_n += len(Xb)\n",
    "\n",
    "    return total_loss / total_n, total_correct / total_n\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, 16):\n",
    "    tr_loss, tr_acc = run_epoch(baseline, train_loader, train=True)\n",
    "    va_loss, va_acc = run_epoch(baseline, val_loader, train=False)\n",
    "\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        best_state = {k: v.cpu().clone() for k, v in baseline.state_dict().items()}\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"[Baseline] Epoch {epoch:02d} | train {tr_loss:.3f}/{tr_acc:.3f} | val {va_loss:.3f}/{va_acc:.3f}\")\n",
    "\n",
    "baseline.load_state_dict(best_state)\n",
    "baseline.to(device)\n",
    "\n",
    "# =====================================================\n",
    "# TEST EVAL\n",
    "# =====================================================\n",
    "test_loss, test_acc = run_epoch(baseline, test_loader, train=False)\n",
    "\n",
    "print(\"\\n=== BASELINE TEST RESULTS ===\")\n",
    "print(f\"Baseline test loss: {test_loss:.4f}\")\n",
    "print(f\"Baseline test acc : {test_acc:.4f}\")\n",
    "\n",
    "# =====================================================\n",
    "# CONFUSION MATRIX\n",
    "# =====================================================\n",
    "baseline.eval()\n",
    "all_true, all_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        Xb = Xb.to(device)\n",
    "        logits = baseline(Xb)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        all_pred.append(preds)\n",
    "        all_true.append(yb.numpy())\n",
    "\n",
    "all_true = np.concatenate(all_true)\n",
    "all_pred = np.concatenate(all_pred)\n",
    "\n",
    "cm_baseline = confusion_matrix(all_true, all_pred, labels=np.arange(n_states))\n",
    "print(cm_baseline.shape)  # should now be (55, 55)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
